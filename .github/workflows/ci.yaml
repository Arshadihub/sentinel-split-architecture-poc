name: sentinel-poc

on:
  push:
    branches: [ main ]

env:
  TF_WORKING_DIR: terraform/envs/us-east-1

jobs:
  ci-deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.8.5

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Terraform Init
        run: terraform init
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Import CloudWatch Log Groups (before plan)
        run: pwsh ./scripts/import-loggroups.ps1

      - name: Cleanup unused AWS resources (free quota)
        run: |
          echo "Cleaning up unused VPCs and EIPs to free quota..."
          
          # Release unattached EIPs
          echo "Releasing unattached Elastic IPs..."
          aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].AllocationId' --output text | tr '\t' '\n' | while read -r allocation_id; do
            if [ -n "$allocation_id" ] && [ "$allocation_id" != "None" ]; then
              echo "Releasing EIP: $allocation_id"
              aws ec2 release-address --allocation-id "$allocation_id" || echo "Failed to release $allocation_id"
            fi
          done
          
          # More aggressive VPC cleanup - target old/unused VPCs
          echo "Deleting unused VPCs..."
          
          # First, list all non-default VPCs for visibility
          echo "Current non-default VPCs:"
          aws ec2 describe-vpcs --query 'Vpcs[?IsDefault==false].[VpcId,Tags[?Key==`Name`].Value|[0],State]' --output table || true
          
          # Delete VPCs without active resources
          aws ec2 describe-vpcs --query 'Vpcs[?IsDefault==false].VpcId' --output text | tr '\t' '\n' | while read -r vpc_id; do
            if [ -n "$vpc_id" ]; then
              echo "Checking VPC: $vpc_id"
              
              # Check if VPC has any instances
              INSTANCE_COUNT=$(aws ec2 describe-instances --filters "Name=vpc-id,Values=$vpc_id" "Name=instance-state-name,Values=pending,running,shutting-down,stopping,stopped" --query 'length(Reservations[].Instances[])')
              
              if [ "$INSTANCE_COUNT" = "0" ]; then
                echo "Attempting to clean VPC: $vpc_id (no active instances)"
                
                # Delete NAT gateways first
                aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$vpc_id" --query 'NatGateways[].NatGatewayId' --output text | tr '\t' '\n' | while read -r nat_id; do
                  [ -n "$nat_id" ] && echo "Deleting NAT Gateway: $nat_id" && aws ec2 delete-nat-gateway --nat-gateway-id "$nat_id" 2>/dev/null || true
                done
                
                # Wait briefly for NAT gateways to delete
                sleep 5
                
                # Delete internet gateways
                aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc_id" --query 'InternetGateways[].InternetGatewayId' --output text | tr '\t' '\n' | while read -r igw_id; do
                  if [ -n "$igw_id" ]; then
                    echo "Detaching and deleting IGW: $igw_id"
                    aws ec2 detach-internet-gateway --internet-gateway-id "$igw_id" --vpc-id "$vpc_id" 2>/dev/null || true
                    aws ec2 delete-internet-gateway --internet-gateway-id "$igw_id" 2>/dev/null || true
                  fi
                done
                
                # Delete subnets
                aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc_id" --query 'Subnets[].SubnetId' --output text | tr '\t' '\n' | while read -r subnet_id; do
                  [ -n "$subnet_id" ] && echo "Deleting subnet: $subnet_id" && aws ec2 delete-subnet --subnet-id "$subnet_id" 2>/dev/null || true
                done
                
                # Delete route tables (except main)
                aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$vpc_id" --query 'RouteTables[?Associations[0].Main!=true].RouteTableId' --output text | tr '\t' '\n' | while read -r rt_id; do
                  [ -n "$rt_id" ] && echo "Deleting route table: $rt_id" && aws ec2 delete-route-table --route-table-id "$rt_id" 2>/dev/null || true
                done
                
                # Delete security groups (except default)
                aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$vpc_id" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text | tr '\t' '\n' | while read -r sg_id; do
                  [ -n "$sg_id" ] && echo "Deleting security group: $sg_id" && aws ec2 delete-security-group --group-id "$sg_id" 2>/dev/null || true
                done
                
                # Finally delete VPC
                echo "Deleting VPC: $vpc_id"
                aws ec2 delete-vpc --vpc-id "$vpc_id" && echo "Successfully deleted VPC: $vpc_id" || echo "Could not delete VPC $vpc_id"
              else
                echo "Skipping VPC $vpc_id (has $INSTANCE_COUNT active instances)"
              fi
            fi
          done
          
          echo "Cleanup complete"

      - name: Install tflint
        run: |
          curl -sSL https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash

      - name: Run tflint
        run: |
          cd ${{ env.TF_WORKING_DIR }}
          tflint --init || true
          tflint

      - name: Terraform Validate
        run: terraform validate
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Plan
        run: terraform plan -out=tfplan
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        run: |
          terraform apply -auto-approve tfplan
          
          # Force update EKS endpoint access for existing clusters
          echo "Updating EKS cluster endpoint access..."
          aws eks update-cluster-config --name eks-backend --resources-vpc-config endpointPublicAccess=true,endpointPrivateAccess=true || true
          aws eks update-cluster-config --name eks-gateway --resources-vpc-config endpointPublicAccess=true,endpointPrivateAccess=true || true
          
          # Wait for endpoint updates and verify AWS access
          echo "Waiting for endpoint updates to complete..."
          sleep 300  # 5 minutes wait
          
          # Verify AWS credentials and access
          echo "Verifying AWS access..."
          aws sts get-caller-identity
          aws eks describe-cluster --name eks-backend --query 'cluster.status'
          aws eks describe-cluster --name eks-gateway --query 'cluster.status'
        working-directory: ${{ env.TF_WORKING_DIR }}

      # Import moved earlier to avoid create conflicts

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Install kubeval
        run: |
          curl -sLO https://github.com/instrumenta/kubeval/releases/latest/download/kubeval-linux-amd64.tar.gz
          tar xzf kubeval-linux-amd64.tar.gz
          sudo mv kubeval /usr/local/bin/
          rm kubeval-linux-amd64.tar.gz

      - name: Check EKS nodegroups before K8s deployment
        id: check-nodegroups
        run: |
          BACKEND_NODEGROUPS=$(aws eks list-nodegroups --cluster-name eks-backend --region us-east-1 --query 'length(nodegroups)' --output text)
          GATEWAY_NODEGROUPS=$(aws eks list-nodegroups --cluster-name eks-gateway --region us-east-1 --query 'length(nodegroups)' --output text)
          echo "backend_nodegroups=$BACKEND_NODEGROUPS" >> $GITHUB_OUTPUT
          echo "gateway_nodegroups=$GATEWAY_NODEGROUPS" >> $GITHUB_OUTPUT
          if [ "$BACKEND_NODEGROUPS" -eq "0" ] || [ "$GATEWAY_NODEGROUPS" -eq "0" ]; then
            echo "WARNING: EKS clusters have no nodegroups yet"
            echo "Skipping K8s deployment until IAM permissions and service quotas are increased"
          fi

      - name: Install eksctl
        run: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Configure EKS cluster access
        if: steps.check-nodegroups.outputs.backend_nodegroups != '0' && steps.check-nodegroups.outputs.gateway_nodegroups != '0'
        run: |
          # Debug current identity and cluster info
          echo "Current AWS identity:"
          aws sts get-caller-identity
          
          echo "EKS cluster info:"
          aws eks describe-cluster --name eks-backend --query 'cluster.{Status:status,Endpoint:endpoint,CreatedBy:tags}'
          aws eks describe-cluster --name eks-gateway --query 'cluster.{Status:status,Endpoint:endpoint,CreatedBy:tags}'
          
          # Try to access cluster directly to see the exact error
          echo "Testing direct cluster access..."
          aws eks update-kubeconfig --name eks-backend --region us-east-1 --alias eks-backend
          kubectl config current-context
          kubectl config view --raw
          
          # Try a simple kubectl command to see the exact error
          echo "Testing kubectl access..."
          kubectl --context eks-backend get ns || echo "Expected failure - cluster not accessible yet"
          
          # Use AWS API directly to create/update aws-auth ConfigMap via EKS API
          echo "Attempting to configure cluster access via AWS API..."
          
          # Get the current aws-auth ConfigMap if it exists
          AWS_AUTH_DATA=$(kubectl --context eks-backend get configmap aws-auth -n kube-system -o jsonpath='{.data}' 2>/dev/null || echo "{}")
          
          # Create a complete aws-auth ConfigMap with both node roles and user mapping
          cat > aws-auth-config.yaml <<EOF
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapRoles: |
              - rolearn: arn:aws:iam::721500739616:role/eks-backend-node-group-20*
                username: system:node:{{EC2PrivateDNSName}}
                groups:
                  - system:bootstrappers
                  - system:nodes
              - rolearn: arn:aws:iam::721500739616:role/eks-gateway-node-group-20*
                username: system:node:{{EC2PrivateDNSName}}
                groups:
                  - system:bootstrappers
                  - system:nodes
            mapUsers: |
              - userarn: arn:aws:iam::721500739616:user/arshadcsinfo@gmail.com
                username: github-actions
                groups:
                  - system:masters
          EOF
          
          echo "Attempting to apply aws-auth configuration..."
          kubectl --context eks-backend apply -f aws-auth-config.yaml || echo "Backend cluster access failed"
          kubectl --context eks-gateway apply -f aws-auth-config.yaml || echo "Gateway cluster access failed"

      - name: Deploy backend manifests to eks-backend
        if: steps.check-nodegroups.outputs.backend_nodegroups != '0'
        run: |
          BACKEND_SG=$(terraform -chdir=${{ env.TF_WORKING_DIR }} output -raw backend_lb_sg_id || true)
          
          # Configure kubectl with proper AWS credentials and region
          export AWS_DEFAULT_REGION=us-east-1
          aws sts get-caller-identity
          aws eks update-kubeconfig --name eks-backend --region us-east-1 --alias eks-backend
          
          # Test authentication
          echo "Testing cluster authentication..."
          kubectl get nodes
          kubectl cluster-info
          
          kubeval k8s/backend/namespace.yaml k8s/backend/deployment.yaml k8s/backend/service.yaml k8s/backend/networkpolicy.yaml || true
          kubectl apply --dry-run=client --validate=false -f k8s/backend/namespace.yaml
          kubectl apply --dry-run=client --validate=false -f k8s/backend/deployment.yaml
          kubectl apply --dry-run=client --validate=false -f k8s/backend/service.yaml
          kubectl apply --dry-run=client --validate=false -f k8s/backend/networkpolicy.yaml
          kubectl apply --validate=false -f k8s/backend/namespace.yaml
          kubectl apply --validate=false -f k8s/backend/deployment.yaml
          if [ -n "$BACKEND_SG" ]; then
            sed "s|__BACKEND_SG__|$BACKEND_SG|g" k8s/backend/service.yaml | kubectl apply --validate=false -f -
          else
            kubectl apply --validate=false -f k8s/backend/service.yaml
          fi
          kubectl apply --validate=false -f k8s/backend/networkpolicy.yaml

      - name: Wait for backend LoadBalancer
        if: steps.check-nodegroups.outputs.backend_nodegroups != '0'
        run: |
          for i in {1..30}; do
            HOST=$(kubectl -n sentinel get svc backend-svc -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            if [ -n "$HOST" ]; then echo "LB=$HOST" && break; fi
            sleep 10
          done
          echo "BACKEND_LB=$HOST" >> $GITHUB_ENV

      - name: Deploy gateway manifests to eks-gateway (patch config with backend LB)
        if: steps.check-nodegroups.outputs.gateway_nodegroups != '0' && steps.check-nodegroups.outputs.backend_nodegroups != '0'
        run: |
          # Configure kubectl with proper AWS credentials  
          export AWS_DEFAULT_REGION=us-east-1
          aws eks update-kubeconfig --name eks-gateway --region us-east-1 --alias eks-gateway
          
          # Test authentication
          echo "Testing cluster authentication..."
          kubectl get nodes
          kubectl cluster-info
          
          kubectl apply --validate=false -f k8s/gateway/namespace.yaml
          cat > nginx.conf <<EOF
          events {}
          http {
            upstream backend {
              server ${BACKEND_LB}:8080;
            }
            server {
              listen 80;
              location / {
                proxy_pass http://backend;
              }
            }
          }
          EOF
          kubectl -n gateway create configmap proxy-conf --from-file=nginx.conf --dry-run=client -o yaml | kubectl apply --validate=false -f -
          kubectl apply --validate=false -f k8s/gateway/deployment.yaml
          kubectl apply --validate=false -f k8s/gateway/service.yaml
